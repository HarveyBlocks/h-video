# H-Video


## TOC

```text
C:\USERS\27970\DESKTOP\IT\JDK\HVEDIO
|   .gitignore
|   pom.xml
|   README.md
|   
+---.idea
|       ...
|
+---imgs
|       img0.png
|       img1.png
|       img2.png
|
+---src
    +---main
        +---java
        |   \---com
        |       \---harvey
        |           \---hvideo
        |               |   HVideoApplication.java
        |               |
        |               +---advice
        |               |       RestfulResponseAdvice.java
        |               |       WebExceptionAdvice.java
        |               |
        |               +---config
        |               |       ApplicationConfig.java
        |               |       MvcConfig.java
        |               |       MybatisConfig.java
        |               |       SecurityConfig.java
        |               |
        |               +---controller
        |               |       FollowController.java
        |               |       UploadController.java
        |               |       UserController.java
        |               |       VideoCommentController.java
        |               |       VideoController.java
        |               |
        |               +---dao
        |               |       FollowMapper.java
        |               |       UserMapper.java
        |               |       VideoCommentMapper.java
        |               |       VideoMapper.java
        |               |
        |               +---exception
        |               |       BadRequestException.java
        |               |       ForbiddenException.java
        |               |       ResourceNotFountException.java
        |               |       UnauthorizedException.java
        |               |
        |               +---interceptor
        |               |       AuthorizeInterceptor.java
        |               |       ExpireInterceptor.java
        |               |       LoginInterceptor.java
        |               |
        |               +---pojo
        |               |   +---dto
        |               |   |       LoginFormDTO.java
        |               |   |       RegisterFormDTO.java
        |               |   |       UserDTO.java
        |               |   |       VideoCommentDTO.java
        |               |   |       VideoDTO.java
        |               |   |
        |               |   +---entity
        |               |   |       Follow.java
        |               |   |       User.java
        |               |   |       Video.java
        |               |   |       VideoComment.java
        |               |   |
        |               |   +---enums
        |               |   |       Role.java
        |               |   |
        |               |   \---vo
        |               |           FileWithUserId.java
        |               |           Null.java
        |               |           Result.java
        |               |           ScrollResult.java
        |               |
        |               +---properties
        |               |       AuthProperties.java
        |               |       JwtProperties.java
        |               |
        |               +---service
        |               |   |   FollowService.java
        |               |   |   UploadService.java
        |               |   |   UserService.java
        |               |   |   VideoCommentService.java
        |               |   |   VideoService.java
        |               |   |
        |               |   \---impl
        |               |           FollowServiceImpl.java
        |               |           UploadServiceImpl.java
        |               |           UserServiceImpl.java
        |               |           VideoCommentServiceImpl.java
        |               |           VideoServiceImpl.java
        |               |
        |               \---util
        |                       Constants.java
        |                       JwtTool.java
        |                       RedisConstants.java
        |                       RedissonLock.java
        |                       RegexPatterns.java
        |                       RegexUtils.java
        |                       TimeUtil.java
        |                       UserHolder.java
        |
        \---resources
            |   application.yml
            |   hmall.jks
            |
            \---static
                    index.html
     

```





# Timeline Feed

## Feed 概述

### 目标

用于推送

- 信息单元(商品/视频/图文)
- 广告



### 要求

- 高可用
- 极低延迟





### 要素

- 召回: 决定哪些 Feed 应该分发给哪些用户

- 排序: 区分展示信息的优先级. 排序规则, 商业性更高



### 分类

Timeline Feed:

- 通过用户与用户之间的关注关系来召回 Feed

- 基于发布时间排序

Top k Feed:

- 依据某些召回策略来召回 Feed

  召回策略

   - HNSW 向量检索

     构建向量点的图, 要求构成连通图, 边尽可能少, 相近的点之间能够直接向量

- 依据推荐模型排序

   - 对点击率的预估做排序
   - 机器学习的模型, 概率有多高, 广告转化率, 贝叶斯

- 复杂的推荐系统



## 设计目标

- 用户发布 Feed
- 用户关注/取消关注其他用户
- 用户查看订阅频道, 看到关注的用户发布的 Feed, 并按照时间排序
- 用户可以点击某个 Feed, 进入到当前 Feed 的详情页面
- 用户可以进入某个用户的个人主页, 展示其曾经发布的 Feed
- 用户的 Feed 有标题/封面/点赞/评论/流量数等信息
- 用户可以点赞评论转发一个 Feed
- 用户可以看自己的收藏/点赞/浏览记录列表, 并~~设置是否公开权限~~
- ~~需要审核机制~~



## 指标

- 延迟
- 吞吐
- 可用性



## 发布 Feed

### 推拉模式

推模式/写扩散: 创作者发布 Feed 后, 将 Feed 写入每一个关注者的 inbox 中(适合在小规模社交系统上进行, 或者是朋友圈这种无法形成大规模粉丝群体的场景)

拉模式: 创作者发布 Feed 后, 将 Feed 写入自己的 outbox, 关注者想要相关信息即可从创作者的 outbox 中获取

推拉结合

- 创作者

   - 普通创作者: 粉丝量少

     策略: 直接采取推模式, 写给每一个用户的 inbox

   - 大 V: 粉丝量多

     策略: 对于核心粉丝, 推模式写 inbox; 对于普通粉丝, 采取拉模式, 写自己的 outbox, 让粉丝到自己的 outbox 去获取

- 关注者

   - 活跃粉丝: 对创作者的 Feed 请求频率高. 在粉丝数量中占比少.
   - 普通粉丝: 对创作者的 Feed 请求频率低, 偶尔请求. 在粉丝数量中占比多.
   - 读取时, 读取自己的关注列表, 获取信息(去 inbox 读还是去 outbox 读), 然后并行去请求信息, 最后合并成结果

**数据结构**: 采取以 timestamp 为 score 的跳表





### 滚动查询

客户端采用滚动更新查看的样式

下一次请求带上上一次 last_id, 下一次查询从 last_id 开始查询



### 下拉刷新

将 last_id 置为空, 表示是查询最新的 Feed



### 用户识别



识别大 V

- 粉丝数量

- 舆情热度(某个人物在某个特定的热点事件中成为了热点人物)计算

- 机器学习预测

- 大 V 不能进行降级

  如果进行降级:

   - 需要对所有的粉丝的 inbox 进行检查
   - 将大 V 的 outbox 中的数据填入粉丝的 inbox
   - 删除大 V 的 outbox

- 大 V 可以分等级, 控制采取推模式的活跃用户占比多少

识别活跃用户

- 活跃程度不同, 收件箱的大小也不同





### 冷热分离

处理inbox中Feed过多达到上限的问题

- 如果用户持续获取 Feed, 直到 inbox 用完, 此时退化成关注者的 outbox 拉取



### 预拉取

加快对用户的响应, 减少延迟对用户的感知

- 在下拉刷新到第 9 次(假设), 接近将本用户的收件箱拉完了, 服务端进行下 10 次的拉取

坏处: 造成了无效的请求

- 因为拉取到 9 次及以上才会触发预拉取, 概率很低, 因此触发预拉取的 QPS 也不高, 所以资源消耗的成本可以接受



### 问题

- 识别大 V 用户时, 可能导致边界问题导致的性能抖动, 如何避免?

   - 使用分级, 阶段性地进行转变

- 大 V 发布 Feed, 所有人去拉大 V 的最新 Feed, 此时大 V 的发件箱所在的服务器宕机了, 怎么办?

   - 建立高可用的集群
   - 读写分离/分片 Redis slot 插槽

- 用户关注的创作者过多, 导致收件箱列表过大, 造成大 Key 问题? 想办法减少key的大小:

   - 收件箱只存储近期的 Feed, 限制大小
   - 采用推拉结合, 不是常关注的大 V, 采用拉模式
   - 采用预拉取的方式

- 对于推模式, 如果一个 Feed 被删除, 如何快速更新?

  比如一个大V删除了自己发的Feed, 这份Feed可能在很多人的inbox里, 如何都删除.

   - 由于升级成了推拉结合, 所以**扇出放大**的影响受到一定控制

   - 并发删除创作者的 outbox 的 Feed 和粉丝的 inbox 的 Feed, 粉丝的部分可以采用合适的 batch

   - 引入一个新的黑名单, 创作者删除时优先将 Feed 标记到黑名单

     在获取 Feed 时, 即使按照前面的逻辑获取到了 Feed, 再次经过黑名单的检查后, 过滤掉黑名单 Feed, 才能返还给客户端

     在 inbox 和 outbox 的 Feed 都确认删除后, 从黑名单中删除 Feed

   - 或者惰性删除, 优先删除活跃用户的 inbox, 对于非活跃用户, 降低删除优先级, 甚至放到下一次登录/查看时检查 inbox, 顺便进行删除

- 收件箱如果写入失败如何处理?

   - 重试

   - MQ 来提高吞吐量并解耦合

     (本来是写缓存的业务, 多一个 MQ, 增加的网络 IO 是否会导致累赘呢? 如果还不存在需求就不要引入 MQ 吧)



RocksDB(ali), KV 存储引擎.

- 依旧遵守 Redis 的协议, 对使用者透明
- 内部基于磁盘
- 性能的部分丢失
- 高效读写权衡, 使用磁盘, 大量分担内存压力, 极大降低硬件成本





# 聚合打包服务

## 详情页设计

### Feed 流的消费数据

粉丝收到热门 Feed, 大量点击阅读, **进入详情页**, 使得详情页 QPS 拉高, 多种下游服务(点赞数, 评论列表, 收藏数等)的整体 QPS 拉高

导致系统性崩溃, 服务不可用

同理还有导致评论数激增, 或产生大量关注行为

### 快速收回

Feed 已经推送给大量用户, 但是这个 Feed 不愿意让用户看到(例如发出的 Feed 是假新闻), 需要快速收回

进行 Delete 操作太过耗时(比如有时由于多级缓存等机制, 无法保证快速删除全域的所有数据), 无法快速收回, 减少损失

方案是缓存一个能够快速写的黑名单, 即使拿到了 Feed, 进入详情页时要检查这个黑名单, 发现 Feed 流在黑名单里, 则快速响应给用户 404

## 聚合打包服务

详情页的工作, 需要聚合多个信息

- 创作者的用户信息(当前用户是否关注该创作者的关系信息)
- 点赞数/收藏数/分享数
- 观看量
- 评论数
- 评论列表

## 旁路缓存

### 缓存三剑客

#### 击穿

- 缓存不存在数据, 大量数据打入数据库
- 互斥锁(引入分布式锁)
- Redisson 看门狗机制, 续时分布式锁
- value 一般是 UUID 表进程 + Thread_ID 表线程
- value 采用 hash 用以实现可重入锁

#### 穿透

- 访问不存在的数据, 导致永远无法缓存, 请求永远打到数据库
- 发现这种请求, 就构造一个空数据到缓存, 下一次访问时访问这个空数据
- 布隆过滤器(insert on 数据库)
   - 假阳性, 布隆过滤器说存在, 可能不存在; 说不存在, 一定不存在.
   - 无法删除, 后期判断失误的可能性极大提高
   - 升级: 布谷鸟过滤器(有利于减少 Hash 冲突)

#### 雪崩

- 宕机导致: 高可用集群
- 大量 key 同时过期导致: 过期时间引入随机值



### 一致性

- 为什么删除缓存而不是更新?: 更改成本高不稳定, 两个线程先后改缓存, 顺序一变结果不一样

- 为什么先写数据库再删缓存?: 如果先删缓存, 别的线程来请求数据库放到缓存, 缓存依旧是旧数据

- 先写数据库再删缓存导致的数据不一致?: 时间较短可以接受

- 如何解决删除缓存的耦合问题?: 使用 Canal, 监听 binlog, 如果发现写操作完成后发消息解耦

- 数据库主从导致缓存不一致的情况: 主库写成功, 删除缓存, 但从库没完成同步, 另一个线程自从库读取到旧数据, 加载旧数据到缓存

  解决方案:

   - 同步双删, 完成写操作后删除一次缓存, 一定时间后再去删除一次缓存, 保证缓存的旧数据不会存在太久(间隔时间难以确认)
   - 在主从之间引入全同步/半同步, 使用 ACK 来保证数据一致性(性能下降)



#### 缓存不一致场景

缓存不一致的场景 1

1. 缓存中无数据, 数据库中有数据 V1
2. 服务 A 从缓存中读数据发现无数据, 于是从数据库中读出 V1
3. 服务 B 将数据库中的数据改成 V2, 删除一次缓存
4. 服务 A 将 V1 写入缓存
5. 此时, 缓存存储数据 V1, 数据库存储数据 V2

Canal 来解耦后就没有这个问题了, 因为是统一用 Canal 来写缓存而不是用来服务缓存

话说服务 A 要去读数据库的数据 V1, 加了==分布式锁(之前用于解决击穿)==, 服务 B 就无法去修改数据库的数据了, 所以这种情况不应该存在....?

缓存不一致问题 2:

1. 缓存中无数据, 数据库中有数据 V1
2. 服务 A 从缓存中读不到数据, 从数据库中读取到数据 V1
3. 服务 B 更改数据库的数据 V1 到 V2, 删除缓存
4. 服务 C 发现缓存中不存在数据, 去读数据库, 读到 V2, 写入缓存
5. 服务 A 将 V1 写入缓存
6. 此时缓存的数据是 V1, 数据库的数据是 V2, 产生不一致的问题

引入==分布式锁(之前用于解决击穿)== 或者 Canal 之后, 这种缓存不一致的情况也不存在

#### 租约

Facebook 的解决方案: **引入租约**

租约解决缓存不一致问题 1:

1. 缓存为空, 数据库中有数据 V1
2. 服务 A 发现缓存为空, 就在缓存中记录一个 token1, 自己保存这个 token1, 然后去数据库读 V1
3. 服务 B 修改数据库的数据 V1 到 V2, 然后删除缓存(同时删除 token)
4. 服务 A 去写缓存, 发现缓存中的 token1 不存在, 更换一个 token, token2 写入缓存, 自己存这个 token2, 然后去数据库读 V2
5. 服务 A 去写缓存, 发现缓存 token2 和自己的 token2 一致, 于是写缓存 V2, 删除缓存中 token2 的记录

租约解决缓存不一致问题 2:

1. 缓存为空, 数据库中有数据 V1
2. 服务 A 发现缓存为空, 就在缓存中记录一个 token1, 自己保存这个 token1, 然后去数据库读 V1
3. 服务 B 更改数据库的数据 V1 到 V2, 删除缓存
4. 服务 C 发现缓存为空, 就在缓存中记录一个 token2, 自己保存这个 token2, 然后去数据库读 V2
5. 服务 C 去写缓存, 发现缓存 token2 和自己的 token2 一致, 于是写缓存 V2, 删除缓存中 token2 的记录
6. 服务 A 写缓存, 发现缓存中没有 token 了, 于是知道缓存中的数据被更新到了更新的版本, 于是从缓存中取得 V2

租约解决击穿:

1. 缓存为空, 数据库中有数据 V1
2. 服务 A 发现缓存为空, 就在缓存中记录一个 token1, 自己保存这个 token1, 然后去数据库读 V1
3. 服务 B 发现缓存为空, 发现记录了 token1, 于是自己等待一会儿, 再去访问, 轮询数次
4. 服务 C 同服务 B
5. 服务 A 发现自己携带的 token1 和缓存中的 token1 一致, 成功将 V1 加载到缓存
6. 服务 B 和 C 发现缓存中没有 token 了, 且已经加载了数据, 就读取缓存

租约相对于分布式锁, 不会阻塞等待(分布式锁阻塞等待是使用了 Redis 的 PubSub 机制, 至少 Redisson 是这么实现的), 但是会不断轮询重试.

当然分布式锁也可以设计成轮询重试的模式, 实现起来也比租约简单一些

### Redis 高可用

- 主从(哨兵集群)
- 分片
- 持久化



#### 持久化

RDB 快照

- 恢复快, 间隔长, 两次间隔之间的数据会丢失, 比较消耗 CPU
- Fork 页表时是阻塞的
- COW, Copy on Write. Fork 页表是阻塞的



AOF 日志

- 指令完成后执行, 底层是 fsync, 配置有 always(阻塞下一次), everysec (丢失 1s, 不阻塞), no



AOF 文件重写机制

- 基于当前的 Redis 已有数据生成指令
- COW, Copy on Write. Fork 页表是阻塞的

- AOF 重写对于 COW 时期的数据不一致如何处理?: 使用 AOF 缓冲区和 AOF 重写缓冲区, COW 期间发生的变化会增量式地写入 AOF 重写后的文件



混合持久化

- 异步采用 RDB
- 两次 RDB 之间(包括进行 RDB 时)采用 AOF
- RDB 完了之后追加 AOF 日志



AOF 重写机制有 **AOF 缓冲区** 和 **AOF 重写缓冲区** 两个缓冲区, 为什么这么设计, 目的呢?

- AOF 缓冲区 + 旧文件形成一个完整备份
- AOF 重写缓冲区和重写的新文件形成一个完整备份
- 假设 AOF 重写失败, 依旧保证数据完整



Redis 7.0 之后删除 AOF 重写缓冲区, 两次重写之间和重写时的指令写入 `.incr.aof` 文件

1. 已经有文件 `1.base` 和 `1.incr.aof` 持久化了数据
2. 开启 `bgrewriteaof`
3. 创建 `2.incr.aof`, 停止写 `1.incr.aof`
4. 进行 fork
5. 子进程异步将内存中的数据 rewrite 到 `2.base`
6. 主进程其他命令进入, 写入 `2.incr.aof`
7. 子进程完成 `2.base`, 删除 `1.incr.aof` 和 `1.base`
8. 主进程继续写入 `2.incr.aof`

如果 AOF 重写失败, `1.base`, `1.incr.aof`, `2.incr.aof` 将一起作为持久化的记录



#### 大 Key 问题

- 不仅是内存损耗, 还有输出缓冲区 buffer(不受淘汰机制控制) 占满(阻塞网络, 阻塞主线程)
- 大 Key(Hash 或集合这种)拆分, Lua 进行聚合. (集群场景下, 希望落在同一个分片, 花括号)
- 分片



#### Hot Key 问题

引入**多级缓存**

- JVM 的 Caffeine
- Nginx 缓存静态资源
- 前端缓存

**双 TTL**, 用于提高可用性

- 一个小缓存内的数据采用一个短 TTL, 例如 10s, 保证数据较强一致性
- 一个大缓存内的数据采用长 TTL, 例如 24h.
- 小缓存的数据过期后, 进入大缓存

1. 当上游服务 A 发现小缓存内无数据时, 访问下游服务 B 获取数据
2. RPC 的过程中失败了
3. 服务 A 去获取大缓存中的较旧数据, 依旧能组建数据



场景:

1. 服务 A 的业务是设置 ItemA, 有自己的缓存 RedisA
2. 服务 X 的业务是聚合 ItemA 和 ItemB.
3. 服务 X 有自己的缓存 RedisX.
4. 为了提高效率, 减少 RPC 带来的性能损耗, 服务 X 将聚合的结果存入了 RedisX 缓存.

**那么问题是**:

1. 缓存RedisX已经有了聚合后的数据ItemA和ItemB
2. 一个修改 ItemA 的请求打入服务 A, 且不经过服务 X
3. 服务 X 无法发现 ItemA 的变化, 导致了数据不一致. 这怎么办?

难点: 前提是 RedisA 和 RedisX 的设置和部署只对对应服务的开发者才了解, 其他开发者是不了解的, 相当于一个黑盒. 服务 A 对 ItemA 进行修改后如果要去修改 RedisX 的数据, 需要知道服务 X 的一系列部署细节, 明显是耦合了, 非常不合适.

解决方案: 使用消息队列, 订阅这个消息, 只要这个项目在项目初期就做好规划和约定, 在合适的地方监听消息队列的消息, 以及时保证缓存的数据一致性. 使用消息队列能够高度解耦, 而不需要知道其他业务的细节

## 客户端预加载

对于用户停留在某个 Feed 流的时候, 客户端异步请求下两次(或多次) Feed 流, 并保存在客户端内存, 提高用户的体验

坏处: 造成了无效的请求

## 无效下游请求

假设详情页服务异步地去请求多个服务, 某个关键服务快速失败, 此时认为整个请求失败.

但是 RPC 对其他服务的请求还是会继续执行下去, 其他服务还有更多子服务, **导致 RPC 资源的浪费**



**解决方法**是希望发现请求失败的时候快速通知其他请求, 统统尽快结束, 避免对资源的占用

如何找到需要打断的目标? 使用链路追踪的技术

如何打断正在进行的服务?

假设服务 A 同时调用服务 B 和服务 C, 服务 A 在阻塞等待两个服务的结果, 服务 B 和服务 C 在执行中

如果服务 B 和服务 C 接受到了打断通知, 正确打断之后, 是否需要通知服务 A 此服务被打断? 不需要, 这个时候说不定服务 A 已经被打断了

那么, 可以异步地打断服务 ABC, 然后不需要服务 ABC 进行返回.



## 频繁写优化

比如点赞数和观看量, 就是会被频繁写的场景, 但其结构又很简单, 只是一个计数

写操作吞吐优先: 要能处理大量的写请求

读操作延迟优先: 不需要读到最新的数据

方案:

- 抽象出一个 CounterServer, 用于这种业务简单, 但是要求吞吐极大的场景
- **变更合并策略**: 引入 MQ 削峰聚合, 比如 100 次观看数加一的请求, 合并成 1 次加 100 的请求





# 大规模IM

## Demo

单机IM Server

轮询策略

## 协议选型

### 服务端推送方案对比

| **方案**      | **原理**                                       | **优点**                                                | **缺点**                                         |
| ------------- | ---------------------------------------------- | ------------------------------------------------------- | ------------------------------------------------ |
| **短轮询**    | 客户端定时发送 HTTP 请求。                     | 实现最简单，兼容性好。                                  | 无效请求多，浪费带宽和 CPU，实时性差。           |
| **长轮询**    | 服务端收到请求后挂起，直到有数据或超时才返回。 | 比短轮询更实时，减少了部分无效请求。                    | 服务端连接堆积，依然有 HTTP Header 的开销。      |
| **WebSocket** | 基于 TCP 的全双工协议，一次握手后持久连接。    | **实时性最高**，Header 极小（节省带宽），支持双向推送。 | 需要维护长连接状态，对服务端并发处理能力有要求。 |

长轮询依旧被动

1.  **客户端发请求**：客户端向服务器发送一个请求，询问是否存在更新。
2.  **服务器不立即回答**：服务器收到请求后，如果没有新数据，就把这个请求**挂起**（不返回响应），一直等到有新数据，或者等到超时时间到了，才返回响应。
3.  **客户端立刻发新请求**：客户端收到响应后，无论拿到的是新数据还是超时通知，都**立刻**发送下一个请求，让服务器继续等待。



**为什么选择 WebSocket 实现聊天模块？**

- **低延迟**：聊天需要及时的反馈，WebSocket 的全双工特性保证了消息能迅速送达。
- **带宽效率**：聊天消息通常短小频繁，WebSocket 避免了 HTTP 频繁握手和冗长的 Header。
- **双向通信**：不仅服务端能推，客户端发送消息也不需要重新建立连接。



在移动端应用上, 需要考虑 IP 变更导致重连的情况. 例如从 Wi-Fi 切换到流量. 需要维护一个 Client Token, 用 token 来定位用户(当然后面考虑到多端, 还要以机器做区分)而不是 IP

### WebSocket 握手过程与身份验证

> **WebSocket 并不是基于 HTTP 的**，但它是**借助 HTTP 来完成握手**的一种独立的 TCP 协议。

**握手交互过程：**

1. **客户端发起 Upgrade 请求**：发送一个标准 HTTP GET 请求，Header 中包含 `Upgrade: websocket` 和 `Connection: Upgrade`，以及一个随机生成的 `Sec-WebSocket-Key`。
2. **服务端响应**：验证请求后，返回 HTTP **101 Switching Protocols**，并根据 Key 生成 `Sec-WebSocket-Accept` 返回。
3. **协议升级**：此后 TCP 连接**不再遵循** HTTP 协议，而是切换到 WebSocket 协议进行二进制帧传输。

**身份验证实现：**

握手本质是 HTTP，因此可以利用 HTTP 的特性：

- **URL 参数**：`ws://example.com/chat?token=xyz`。
- **自定义 Header**：在发起握手请求时带上 `Authorization`。
- **Cookie**：如果是在浏览器环境下，握手请求会自动携带同域 Cookie，服务端在拦截器中校验即可。



## 服务拆分

- Netty 网关, 解析websocket协议
- IM 服务
- 连接状态服务
- 路由转发服务
- 离线消息处理的服务



## 单点聊天

## 引入网关

1. Nginx 做四层LB TCP 连接转发负载均衡
2. Netty 网关解析 Websocket 协议转为RPC通信
3. IM 服务处理具体业务



### 找到目标连接(路由层)

WebSocket 的两个 session 在不同节点, 这两个 session 需要找到对方, 则需要发现对方的连接

方案一: 广播

广播一个消息给所有的服务, 然后服务检查自己持有的长连接中有无目标连接, 无目标连接则抛弃

缺点是

- 容易导致消息风暴
- 单聊场景会产生过多的无效通信



方案二: 一致性 Hash

使用一个服务发现系统, 管理一个 Hash 环, 管理各个服务器.

能够通过目标用户的一些信息, 直接找到目标用户所在的服务器

实现简单, 计算简单

缺点是

- 对于水平扩展存在需要迁移的情况, 对于迁移的连接通过断线重连的方式实现
- 依赖 Hash 算法的均匀性限制



方案三: 维护转发表

WebSocket 的 session 在不同节点 -> 通过 Redis 维护转发表, 记录不同用户在节点上的信息, 进行查询, 然后请求

所有的映射都在路由层维护, 可靠性高, 路由服务无状态且可水平扩展

缺点是实现复杂



### 发现对方连接后

架构一: **用户亲和**

如果两个用户挂在不同的服务器上, 经过查询知道另一个用户的位置, 然后移动另一个用户到本服务器上

架构二

用户直接挂在自己的服务器上, 然后通过一个转发表 + 消息队列, 对消息进行转发

当然可以先检查对方是否在本机, 如果不在再去查询转发表. (因为一般是去最近的服务器上进行连接的, 同机率比较高)

多了一次转发, 延迟会高一些. 但是能分摊热度, 有助于水平扩展

需要转发表的一致性高一些, 也要考虑消息的顺序性/幂等性问题



### 离线

离线箱

- 保存在一个 log file, 追加写, 效率可以接受
- 采用 NoSQL 或者 KV 存储

离线箱的信息长时间不被用户获取, 就认为用户接下来更长时间也不会去获取了, 就落库(此时效率也不再是第一要义了)



## 消息可靠性

聊天系统本质上是三方通信，消息端到端的可靠性 = 上行消息可靠 + 服务端业务可靠 + 下行消息可靠

1. **上行可靠性**：客户端发送后开启定时器，若未收到服务端的 **应用层 ACK**，则触发重试。
2. **服务端业务可靠**：消息到达服务端后先持久化（入库/缓存），再给发送方回 ACK，防止后续推送失败导致消息丢失。
3. **下行可靠性**：服务端推送给接收方，接收方收到后回 ACK。若服务端未收到，则将消息标记为“未读”，待接收方下次上线时拉取。



### 时序基准



观察 QQ 发现, 可能出现一种情况:

1. 自己的消息
2. 对方的消息

最终退出重进后展示的:

1. 对方的消息
2. 自己的消息

且后面再重进时保持这个顺序不变



- 采用时间戳?

   - 问题在于没有一个全局时钟, 机器的时钟存在差异, 或者说时钟偏移
   - 客户端生成? 不可靠, 可能被改
   - 服务端生成? 网络延迟下, 单独的时间戳依旧不可靠

- 客户端引入序列号?

   - 一个用户一个会话一个序列号

   - 保证单个用户的前后语言的逻辑一致性

   - 但是引入了序列号之后, 还需要时间戳吗?

     我认为是不需要了, 时间戳也是用于保证单个用户的消息前后逻辑, 但时间戳不如序列号

     时间戳顶多处理一下绕回, 像 TCP 的 PAWS 一样, 但 IM 系统里不会绕回的

     腾讯还是采用了时间戳, 然后同秒内序列号. 但我认为光序列号够用了, 不需要时间戳的

   - 客户端可能由于应用重装导致**序列号清零**: 服务端稍微维护一下这个**序列号**, 产生还是由客户端, 但服务端也做相应记录

- 不同用户之间的消息顺序的一致性如何保证, 每个用户看到的消息顺序是一致的

   - USER_ID 的比对? 有缺陷, 某个用户 ID 比较靠前, 就可以让他的消息总是靠前吗?
   - 采用**全局递增 ID 生成器**(真是全局吗? 其实一个单点对话/一个群聊对应一个 ID 生成器, 是这样的粒度吧?)



1. 客户端采用序列号, 保证同一个用户的消息顺序性

2. 对于同一个用户, 依次向一个全局递增 ID 生成器请求

   也可以批操作, 比如一次要给 N 个消息请求 N 个 ID, ID 往后 skip N 位

3. 此次操作的所有消息都有了自己的消息 ID, 服务端保存**此用户的最大序列号**

4. 此时不同用户之间的消息也存在了顺序, 同时保证了不同用户看起来的消息顺序也是一致的

5. 生成的消息 ID 也要返回给用户客户端

### 整包发送



- 客户端将短时间内的多个消息打包合并发送
- 但依旧要考虑包和包之间的顺序问题
- 但是能够大大降低出现收到乱序请求的概率, 减轻服务端的运算压力
- 可以引入对包的压缩, 包越大, 压缩的效果越好





### 存在"引用"的消息

其他的, 要保证**因果一致性**的情况:

比如有存在"引用"消息的需求的话, 另外保证顺序性

- 如果引用的消息是对方的, 则一定存在消息 ID, 则消息一定已经被发出, 不必担心

- 如果引用的消息是自己的, 且存在消息 ID, 同理, 不必担心

- 如果引用的消息是自己的, 且还没有消息 ID, 那么看序列号

  (客户端能保证引用消息的序列号一定在本条消息之前)

   - 如果序列号此时在服务器上, 说明是同一批, 那么可以获取到消息 ID 并替换序列号

   - 如果序列号此时不在服务器上

     情形 A: 可能是已经获取了消息 ID, 被发送出去了的前一批, 但客户端在构造这一条消息的时候还不知道已经有消息 ID

     情形 B: 可能是由于网络延迟导致消息还没来

     查看服务器存储的**此用户的最大序列号:**

     如果大于引用消息的序列号, 说明是情形 B, **等待**引用消息的到来

     否则, 说明是情形 A, **则向客户端请求这个序列号对应的消息的消息 ID**, 然后来替换序列号

### 全局 ID 自增生成器

考虑全局 ID 自增生成器的**单点瓶颈**

直接看粒度是业务上的, 但可以进一步细分, 例如大多数业务是用户隔离的, 那么粒度就是这个业务的这个用户上的 ID 自增, 那就不是那么瓶颈了

聊天的场景, 粒度就是会话 ID(单点对话的 ID 或群聊的 ID), 群聊一般会限制用户人数, 两千人已经是大群了, 大群也是大部分潜水的, 主要是小部分人在聊天, 其实也不会很极限

然后也没必要粒度特别细, 可以使用 hash, 几个不同的群聊/单点使用同一个 ID 生成器, 减少存储资源的损耗

如果实在单点瓶颈. 那么考虑**雪花算法*



**ID 生成器高可用**

创建冗余副本

但是如果在主节点发生自增后宕机, 而自增没有被写到副本里去, 将不能保证 ID 依旧能够自增

解决方案是在启用副本时, 给 ID 加上一定的跨度, 保障 ID 是自增的



### 幂等性保证

幂等性的业务如查询, 删除. 但是性能有损耗

非幂等性的业务有下单和扣减库存.

保证幂等性的策略, 可以依据业务本身的特性, 比如下单可以引入订单状态, 未下单到成功下单的状态只能进行一次.

在 IM 系统中的消息收发, 要保证幂等性**使用 ID**

白名单: 消费者保存已经处理过的 ID, 下一次消息的 ID 如果已经存在, 则丢弃

黑名单: 生产者保存要发送的 ID 存起来, 生产者处理完消息后, 将 ID 从存储中删除; 下一次生产者收到消息如果不在存储中, 则丢弃

一般正向的思路上采用白名单, 黑名单在维护起来更复杂

如果维护黑名单或者白名单在 Redis 这个单线程的系统里的话, 不用担心并发问题; 但如果不是, 也是要考虑一下并发问题的(比如使用 select for update 了)

白名单的话, 数据增长没有上限, 只能采用定时清理旧的消息记录来保证存储可控(或者是直接新 key 替换旧 key)

但黑名单, 就好像滑动窗口一样, 大小还是比较可预测的



### 双 ID 链方法

不依赖 ID 生成的单调性, 不需要全局的 ID 生成

1. 消息携带两个 ID: 当前消息的 ID 和其前一个消息的 PRE_ID

2. 服务端存储 PRE_ID

3. 对于一个新消息, 对比 PRE_ID 和服务器记录

   如果 PRE_ID 相等

   - 表示不重不漏
   - 消息通过后, 将新消息的 PRE_ID 存储到服务端

   如果 PRE_ID 不一致

   - 将服务器记录的那个 PRE_ID 返回
   - 发送方回退到 PRE_ID 的位置, 重置所有的消息心跳然后重新发送

缺点是双方通信次数过多(需要将 PRE_ID 返回), 群聊性能差

只适合上行消息(客户端到服务端), 不适合下行消息(服务端到客户端)

下行消息可以使用**推拉结合**的策略



### TCP 失掉可靠力了吗 ?

- TCP 是工作在传输层上的, 网卡对网卡的, 应用程序挂了, 内核能正确响应, TCP 会直接断开而不是重发
- 乱序不是网络导致的, 是应用导致的, 比如生产者和消费者在多线程处理时导致
- TCP 无法处理逻辑上的“超时”（比如对方离线）。
- IM 是 `Client A -> Server -> Client B`。TCP 只能保证 A 到 Server 的可靠性，不能保证 Server 逻辑处理后转发给 B 也可靠。



## 群聊

1. 发送一条群聊消息

2. 依据群聊 ID 获取所有成员的用户 ID

   ```sql
   select member_id
   	from tb_group_members
   	where group_id = #{group_id};
   ```

3. 依据所有成员的用户 ID 去转发表获取

   ```sql
   select distinct server_id 
   	from tb_forward
   	join (
           select member_id
           	from tb_group_members
           	where group_id = #{group_id}
       ) as tb_members on tb_members.member_id = tb_forward.user_id
   ```

   获取到多个 server_id 之后, 将消息

   ```json
   {
       "group_id": "...",
       "content": "",
       "from_user_id": "",
       "timestamp": "",
       "message_id": "",
   }
   ```

   群发到各个 server 上.

4. 每个 server 收到自己的消息后, 各自处理. 依据 group_id, 在本机上查找属于这个 group_id 的用户, 然后向用户转发消息





### 大群突发流量

比如群主说了一句话, 大家争相响应, 大量请求进入服务器, 客户端感官上就是大量记录过去而看不清

此时, 系统的约束发生了变化, 我们不再希望**延时**依旧是第一要义, 反而应该**吞吐量优先**

而且, 大群出现突发流量, 存在**热点**的特征, 因为一个群里的大家看的都是同样的资源, 可能会产生一样的行为

此时消息的回执 (用于显示消息未读已读) 会产生大量写操作, 如果是旁路缓存的策略, **缓存将一直被删除失效**

可能会导致一些本来 QPS 比较低的业务, 由于大群的突发流量, 被打崩, 例如群里来了新成员是一个大 V, 导致一群人去查看群成员列表, 然后去做加好友的操作

### 大群优化策略

大群共有 M 个人在线, 其中 N 个人在发消息(假设每人平均每秒发送 X 条消息), 那么就会存在每秒 $M\times N \times X$ 个消息收发, 引发消息风暴

优化方向:

把一部分消息($N \times X$)合并发送给用户, 采用推拉结合的策略

1. 服务端接收到这个群组发来的多个消息, 在一定时间内进行整理和排序后放入 DB(for example)
2. 向需要接收消息的用户发送一个 Pull 请求
3. 用户向服务器发起请求, 拉取多个还未收到的消息, 降低了消息的传输

回执消息, 即报告此用户对此消息的已读状态, 在大群的场景下也涉及大量的写请求

- 回执消息的特性, 就是对一致性的要求不高, 可以异步进行
- 对聊天室进行热度的区分, 对于热度高的聊天室, 优先级降低, 服务降级, 例如从推模式转为拉模式

禁用心跳, 减少数据传输次数



对于大量信息, 用户一下子看不过来, 可能会向上拉, 于是可以采用预拉取:

- 客户端在拉取第一次信息包之后, 提前异步地去拉取几个下面的信息包
- 服务端也可以保留几个较旧的数据在缓存



## 是否保存数据

- 企业聊天
   - 应当保存较长的时间
- 用户的聊天信息
   - 考虑用户的隐私
   - 降低泄露的风险
   - 尽可能少地保存用户的聊天记录

可以把每条消息抽象成特征向量, 然后存储, 可用于后续的统计和分析, 而消除了个人的敏感信息(大概)





## API over WebSocket

已经建立了 WebSocket 长连接了, 一般的请求能复用这个连接吗?

困难: WebSocket 是有状态的, 不是请求响应模型的了, 因此此时需要自己维护整个请求响应模型, 要求请求和响应进行对应

**STOMP 协议**，本质上是在 WebSocket 之上定义了一套文本协议，自然支持“请求-响应”模式



## 多端在线

对于路由的转发

- 自己发送的消息, 要同时推送到自己其他端所在的服务器上

对于顺序性的保证: 思路依旧是想办法从全局 ID 生成器里去正确地拿 ID

- 应当看作不同的用户在发送消息, 那么用用户 ID 做区分的部分应当改成客户端的机器 ID 做区分了

- 那么即使在单点的聊天里, 也要看作群聊, 不同设备的同用户, 也要看作是另一个用户一样发送消息

## Pub/Sub 路由

Pub/Sub 是 Redis 的一种发布订阅模型, 基本的点对点消息模型

- 一个消费者可以订阅一个或多个 channel(频道)

- 所有订阅者都能接收到订阅的 channel 的相关消息

- 天生就是阻塞式的

- 优点

   -   支持多生产, 多消费

- 缺点

   - 不支持数据持久化

     生产者不保存数据, 没有消费者消费消息, 消息将被丢失

   - 无法避免消息丢失

   - 消息堆积有上限, 超出时数据丢失

     消费者处理消息如果超时, 下一条消息纷至沓来, 超出上限就会丢失

对于 Pub/Sub 的消息丢失, 可以利用 IM 系统的 Seq, 主动向生产者请求被丢失的数据

Redis 的另外的实现有 Stream, 但是 Stream 会存储消息(通过日志追加写), 偏重量, 效率偏低, 仅路由的话是不需要的



## 长连接高可用

长连接存在的问题

长连接是跨越公网的

内网来说, 长连接断联的可能性低; 而经过 NAT 公网, 经过不同运营商的网络, 可能存在断联

### 心跳保活

- TCP 的 keepalive 心跳机制
   - 默认两小时
   - 运营商会对长时间(5min)不收发消息的连接会进行强制断开
- 客户端来发送心跳
   - 服务端来发送心跳, 就要维护所有连接的定时任务, 几乎不可用
   - 服务端接收到心跳后, 对连接的计时器做一个重置
   - 如果服务端长时间收不到心跳, 就会把连接断开, 节省资源
- 控制心跳包的大小
   - 超时时间
   - 包是第几次重试
   - 用于网络状态的统计
- 权衡心跳的时间间隔
   - 心跳过长, 服务端感知断线客户端的效率越低, 资源利用率越低
   - 心跳过短, 造成**心跳潮汐**, 给网关造成流量压力
- 不同运营商的**NAT 淘汰时间**不一样, 心跳应该比最短的那个时间小一些
   - 客户端采用固定心跳, 采用测算好了的值
   - 后台状态(服务之间的内网连接)采用自适应心跳包, 对 NAT 淘汰时间进行测算(二分法)
   - 引入随机值, 减少网络潮汐的可能性



### 断线重连

移动端在移动的时候可能切换网络, 导致**断线重连**

希望重连之后能够连接在同一个节点上, 因为节点上的有关数据不一定是第一时间进行删除的(如果第一时间删除对 CPU 的消耗较大). 希望能够复用之前连接的有关信息



如果移动速度较快, 可能导致**发生多次断线重连**

可以让客户端将几次快速的重连合并成最终一次, 控制客户端在一段时间内仅发起一次重连请求, 减轻服务端的压力

“节流”或“防抖”的思想

- 节流: 限制函数在一定时间间隔内执行一次，无论事件触发有多频繁。
- 防抖: 将多次执行合并为一次，只有在最后一次触发事件后，延迟一定时间没有新的触发，才执行操作。

或者**退化成短链接**, 去拉取信息而不是收发即时消息





**服务端宕机**导致大量客户端重新连接到另一台机器上(另一台机器要突然处理这么多重连, 也可能被压垮)

解决方案是引入所有 IMServer 节点的注册中心, 能及时发现有节点宕机并能够将重连的请求随机打散



### 消息风暴

WebSocket 作为有状态的长连接, 如果服务上关联了大量连接, 随之而来的是大量对连接状态的管理

如果服务宕机, 导致大量连接状态的丢失, 那么重连之后需要**重新建立状态**, 造成性能的损耗

- 引入 State 服务, 专门用于存储连接的状态, 与 WebSocket 直接连接的服务不存储 State 信息
- State 可以做一定持久化处理, 比如使用**快照+Checkpoint**
- 但是恢复的时候, 对时效性很强的信息(比如计时器), 无法恢复完全



大量定时器占用大量内存资源, 并且定时任务的触发会使得整个系统**卡顿造成消息超时**

- 传统的定时任务采用二叉堆实现(小根堆, 获取最近的任务然后等待, 执行), 增删的复杂度是 LogN

- 采用**时间轮**, 可以常量级别的插入和删除, 但定时精度有所丢失(业务不太需要太高精度)

   - 认为任务的延时分布是均匀的
   - 时间轮有固定个数的插槽, 每个插槽是链表
   - 链表上有多个节点存储任务和"**圈数**"

  插入

   1. 新增任务时执行一次 **运算** 找到插槽

      计算出槽位的偏移量
      $$
      任务延时跨越的槽位数量=\frac{任务的延迟时间}{每个槽位的时间跨度}
      $$
      计算出槽位
      $$
      槽位 = 任务延时跨越的槽位数量\mod 总槽位数
      $$

2. 记录圈数为
   $$
   圈数 = \frac{当前插槽+任务延时跨越的槽位数量}{总槽位数}
   $$

3. 将节点放入插槽

查找下一个任务

1. 指针往下一个插槽移动
2. 遍历插槽的链表, 执行此时插槽中所有圈数为 0 的任务
3. 指针每转过一圈，所有任务的圈数减 1。

放入和删除都是 O(1) 的





# Elasticsearch 复杂查询

需求分析:

需要对标题进行分词查询, 进行复杂查询, 排序的结果是需要综合考虑多方面的

- 关键词的匹配度
- 作品的热门程度, 以点击量为例
- 作品发布的时间
- 用户喜好的向量

可能出现的问题:

- 数据同步
   - 简单使用双写来实现: 代码侵入, 效率低
   - 异步双写, 引入消息队列: 解耦
   - 引入 Canal 最终同步
- 搭建集群, 一主多从:
   - 脑裂: 选举票超过半数, 裂成两半, 节点多的哪个集群产生下一个主

Elasticsearch 的集群和分片

就是一台机器上存储多个分片, 分片存在冗余

Elasticsearch A 存在分片 1 2

Elasticsearch B 存在分片 2 3

Elasticsearch C 存在分片 1 3

那么任意机器宕机, 都保证能够复原所有分片的数据, 保证至少有一个分片的数据还在



# 总结

## 分级存储架构

- Nginx (CDN 静态资源存储)
- JVM (Caffeine)
- 双 TTL 缓存
   - Redis (小缓存)
   - Redis (大缓存, 甚至 RocksDB)
- Elasticsearch (复杂查询)
- NoSQL (快速存储, 注重读写, 容许弱一致性, 最终一致)
- 关系型数据库 (最终落库, 不在意效率, 允许较大事务, 在意一致性)

