# H-Video


## TOC

```text
C:\USERS\27970\DESKTOP\IT\JDK\HVEDIO
|   .gitignore
|   pom.xml
|   README.md
|   
+---.idea
|       ...
|
+---imgs
|       img0.png
|       img1.png
|       img2.png
|
+---src
    +---main
        +---java
        |   \---com
        |       \---harvey
        |           \---hvideo
        |               |   HVideoApplication.java
        |               |
        |               +---advice
        |               |       RestfulResponseAdvice.java
        |               |       WebExceptionAdvice.java
        |               |
        |               +---config
        |               |       ApplicationConfig.java
        |               |       MvcConfig.java
        |               |       MybatisConfig.java
        |               |       SecurityConfig.java
        |               |
        |               +---controller
        |               |       FollowController.java
        |               |       UploadController.java
        |               |       UserController.java
        |               |       VideoCommentController.java
        |               |       VideoController.java
        |               |
        |               +---dao
        |               |       FollowMapper.java
        |               |       UserMapper.java
        |               |       VideoCommentMapper.java
        |               |       VideoMapper.java
        |               |
        |               +---exception
        |               |       BadRequestException.java
        |               |       ForbiddenException.java
        |               |       ResourceNotFountException.java
        |               |       UnauthorizedException.java
        |               |
        |               +---interceptor
        |               |       AuthorizeInterceptor.java
        |               |       ExpireInterceptor.java
        |               |       LoginInterceptor.java
        |               |
        |               +---pojo
        |               |   +---dto
        |               |   |       LoginFormDTO.java
        |               |   |       RegisterFormDTO.java
        |               |   |       UserDTO.java
        |               |   |       VideoCommentDTO.java
        |               |   |       VideoDTO.java
        |               |   |
        |               |   +---entity
        |               |   |       Follow.java
        |               |   |       User.java
        |               |   |       Video.java
        |               |   |       VideoComment.java
        |               |   |
        |               |   +---enums
        |               |   |       Role.java
        |               |   |
        |               |   \---vo
        |               |           FileWithUserId.java
        |               |           Null.java
        |               |           Result.java
        |               |           ScrollResult.java
        |               |
        |               +---properties
        |               |       AuthProperties.java
        |               |       JwtProperties.java
        |               |
        |               +---service
        |               |   |   FollowService.java
        |               |   |   UploadService.java
        |               |   |   UserService.java
        |               |   |   VideoCommentService.java
        |               |   |   VideoService.java
        |               |   |
        |               |   \---impl
        |               |           FollowServiceImpl.java
        |               |           UploadServiceImpl.java
        |               |           UserServiceImpl.java
        |               |           VideoCommentServiceImpl.java
        |               |           VideoServiceImpl.java
        |               |
        |               \---util
        |                       Constants.java
        |                       JwtTool.java
        |                       RedisConstants.java
        |                       RedissonLock.java
        |                       RegexPatterns.java
        |                       RegexUtils.java
        |                       TimeUtil.java
        |                       UserHolder.java
        |
        \---resources
            |   application.yml
            |   hmall.jks
            |
            \---static
                    index.html
     

```


# 大规模IM

## Demo

单机IM Server

轮询策略

## 协议选型

SSE: 用户发送一次请求, 服务端来进行后续的多次response, 全部发完后断开连接

短轮询(性能损耗, 大部分时间是浪费)

长轮询(时效性糟糕)

websocket

在移动端应用上, 需要考虑IP变更导致重连的情况. 例如从 Wifi 切换到流量. 需要维护一个Client Token, 用token来定位用户(当然后面考虑到多端, 还要以机器做区分)而不是IP

QUIC 支持用户在不同网络之间切换



服务拆分

- 聊天服务
- 当前在线用户转发表的查询和维护业务的服务
- 维护群聊信息的服务
- 维护单点聊天信息的服务
- 用户信息的服务
- 离线消息处理的服务



## 单点聊天

### 找到目标连接(路由层)

websocket的两个session在不同节点, 这两个session需要找到对方, 则需要发现对方的连接

方案一: 广播

广播一个消息给所有的服务, 然后服务检查自己持有的长连接中有无目标连接, 无目标连接则抛弃

缺点是

- 容易导致消息风暴
- 单聊场景会产生过多的无效通信



方案二: 一致性Hash

使用一个服务发现系统, 管理一个Hash环, 管理各个服务器.

能够通过目标用户的一些信息, 直接找到目标的用户所在的服务器

实现简单, 计算简单

缺点是

- 对于水平扩展存在需要迁移的情况, 对于迁移的连接通过断线重连的方式实现
- 依赖Hash算法的均匀性限制



方案三:  维护转发表

websocket的session在不同节点->通过redis维护转发表, 记录不同用户在节点上的信息, 进行查询, 然后请求

所有的映射都在路由层维护, 可靠性高, 路由服务无状态可可水平扩展

缺点是实现复杂



### 发现对方连接后

架构一: **用户亲和**

如果两个用户挂在不同的服务器上, 经过查询知道另一个用户的位置, 然后移动另一个用户到本服务器上

架构二

用户直接挂在自己的服务器上, 然后通过一个转发表+消息队列, 对消息进行转发

当然可以先检查对方是否在本机, 如果不在再去查询转发表. (因为一般是去最近的服务器上进行连接的, 同机率比较高)

多了一次转发, 延迟会高一些. 但是能分摊热度, 有助于水平扩展

需要转发表的一致性高一些, 也要考虑消息的顺序性/幂等性的问题



### 离线

离线箱

- 保存在一个log file, 追加写, 效率可以接收
- 采用NoSQL或者KV

离线箱的信息长时间不被用户获取, 就认为用户接下来更长时间也不会去获取了, 就落库(此时效率也不再是第一要义了)



## 消息顺序性

观察QQ发现, 可能出现一种情况:

1. 自己的消息
2. 对方的消息

最终退出重进后展示的:

1. 对方的消息
2. 自己的消息

且后面再重进时保持这个顺序不变



### 时序基准

- 采用时间戳?

   - 问题在于没有一个全局时钟, 机器的时钟存在差异, 或者说时钟偏移
   - 客户端生成? 不可靠, 可能被改
   - 服务端生成? 网络延迟下, 单独的时间戳依旧不可靠

- 客户端引入序列号?

   - 一个用户一个会话一个序列号

   - 保证单个用户的前后语言的逻辑的一致性

   - 但是引入了序列号之后, 还需要时间戳吗?

     不需要了啊感觉, 时间戳也是用于保证单个用户的消息前后逻辑, 但时间戳不如序列号

     时间戳顶多处理一下绕回, 像TCP的PAWS一样, 但IM系统里不会绕回的

     腾讯还是采用了时间戳, 然后同秒内序列号. 但我认为光序列号够用了, 不需要时间戳的

   - 客户端可能由于应用重装导致**序列号清零**: 服务端稍微维护一下这个**序列号**, 产生还是由客户端, 但服务端也做相应记录

- 不同用户之间的消息的一致性如何保证

   - USER_ID的比对? 有缺陷, 某个用户ID比较靠前, 就可以让他的消息总是靠前吗?
   - 采用**全局递增ID生成器**(真是全局吗? 其实一个单点对话/一个群聊对应一个ID生成器, 是这样的粒度吧?)

1. 客户端采用序列号, 保证同一个用户的消息的顺序性

   如果长时间等不到旧时消息, 一定时间后丢弃, 选择不继续等待, 然后报告客户端错误, 保证不会导致系统长时间等待

   微信有时候会有这种网络不好发不出去的情况, 就会报告错误, 但后面的消息依旧能发出去

2. 对于同一个用户, 依次向一个全局递增ID生成器请求

   也可以批操作, 比如一次要给N个消息请求N个ID, ID往后skip N位

3. 此次操作的所有消息都有了自己的消息ID, 服务端保存**此用户的最大序列号**

4. 此时不同用户之间的消息也存在了顺序, 同时保证了不同用户看起来的消息顺序也是一致的

5. 生成的消息ID也要返回给用户客户端

### 整包发送

- 客户端将短时间内的多个消息打包合并发送
- 但依旧要考虑包和包之间的顺序问题
- 但是能够大大降低出现收到乱序请求的概率, 减轻服务端的运算压力
- 可以引入对包的压缩, 包越大, 压缩的效果越好





### 消息引用的需求

其他的, 要保证**因果一致性**: 比如有存在"消息引用"的需求的话, 另外保证顺序性

- 如果引用的消息是对方的, 则一定存在消息ID, 则消息一定已经被发出, 不必担心

- 如果引用的消息是自己的, 且存在消息ID, 同理, 不必担心

- 如果引用的消息是自己的, 且还没有消息ID, 那么看序列号

  (客户端能保证引用消息的序列号一定在本条消息之前)

   - 如果序列号此时在服务器上, 说明是同一批, 那么可以获取到消息ID并替换序列号

   - 如果序列号此时不在服务器上

     情形A: 可能是已经获取了消息ID, 被发送出去了的前一批, 但客户端在构造这一条消息的时候还不知道已经有消息ID

     情形B: 可能是由于网络延迟导致消息还没来

     查看服务器存储的**此用户的最大序列号:**

     如果大于引用消息的序列号, 说明是情形B, **等待**引用消息的到来

     否则, 说明是情形A, **则向客户端请求这个序列号对应的消息的消息ID**, 然后来替换序列号

### 全局ID自增生成器

考虑全局ID自增生成器的**单点瓶颈**

直接看粒度是业务上的, 但可以进一步细分, 例如大多数业务是用户隔离的, 那么粒度就是这个业务的这个用户上的ID自增, 那就不是那么瓶颈了

聊天的场景, 粒度就是会话ID(单点对话的ID或群聊的ID), 群聊一般会限制用户任务, 两千人已经是大群了, 大群也是大部分潜水的, 主要是小部分人在聊天, 其实也不会很极限

然后也没必要粒度特别细, 可以使用hash, 几个不同的群聊/单点使用同一个ID生成器, 减少存储资源的损耗

如果实在单点瓶颈. 那么考虑**雪花算法**

**ID生成器高可用**

创建冗余副本

但是如果在主节点发生自增后宕机, 而自增没有被写到副本里去, 将不能保证ID依旧能够自增

解决方案是在启用副本时, 给ID加上一定的跨度, 保障ID是自增的



### 幂等性保证

幂等性的业务如查询, 删除. 但是性能有损耗

非幂等性的业务有下单和扣减库存.

保证幂等性的策略, 可以依据业务本身的特性, 比如下单可以引入订单状态, 未下单到成功下单的状态只能进行一次.

在IM系统中的消息收发, 要保证幂等性**使用ID**

白名单: 消费者保存已经处理过的ID, 下一次消息的ID如果已经存在, 则丢弃

黑名单: 生产者保存要发送的ID存起来, 生产者处理完消息后, 将ID从存储中删除; 下一次生产者收到消息如果不在存储中, 则丢弃

一般正向的思路上采用白名单, 黑名单在维护起来更复杂

如果维护黑名单或者白名单在Redis这个单线程的系统里的话, 不用担心并发问题; 但如果不是, 也是要考虑一下并发问题的(比如使用select for update了)

白名单的话, 数据增长没有上限, 只能采用定时清理掉旧的消息记录来保证存储可控了(或者是直接新key替换旧key)

但黑名单, 就好像滑动窗口一样, 大小还是比较可预测的

### 双ID链方法

不依赖ID生成的单调性, 不需要全局的ID生成

1. 消息携带两个ID: 当前消息的ID和其前一个消息的PRE_ID

2. 服务端存储PRE_ID

3. 对于一个新消息, 对比PRE_ID和服务器记录

   如果PRE_ID相等

   - 表示不重不漏
   - 消息通过后, 将新消息的PRE_ID存储到服务端

   如果PRE_ID不一致

   - 将服务器记录的那个PRE_ID返回
   - 发送方回退到PRE_ID的位置, 重置所有的消息心跳然后重新发送

缺点是双方通信次数过多(需要将PRE_ID返回), 群聊性能差

只适合上行消息(客户端到服务端), 不适合下行消息(服务端到客户端)

下行消息可以使用**推拉结合**的策略



## 群聊

1. 发送一条群聊消息

2. 依据群聊ID获取所有成员的用户ID

   ```sql
   select member_id
   	from tb_group_members
   	where group_id = #{group_id};
   ```

3. 依据所有成员的用户ID去转发表获取

   ```sql
   select distinct server_id 
   	from tb_forword
   	join (
           select member_id
           	from tb_group_members
           	where group_id = #{group_id}
       ) as tb_members on tb_members.member_id = tb_forword.user_id
   ```

   获取到多个server_id之后, 将消息

   ```json
   {
       "group_id": "...",
       "content": "",
       "from_user_id": "",
       "timestamp": "",
       "message_id": "",
   }
   ```

   群发到各个server上.

4. 每个server收到自己的消息后, 各自处理. 依据group_id, 在本机上查找属于这个group_id的用户, 然后向用户转发消息





### 大群突发流量

比如群主说了一句话, 大家争相响应, 大量请求进入服务器, 客户端感官上就是大量记录过去而看不清

此时, 系统的约束发生了变化, 我们不再希望**延时**依旧是第一要义, 反而应该**吞吐量优先**

而且, 大群出现突发流量, 存在**热点**的特征, 因为一个群里的大家看的都是同样的资源, 可能会产生一样的行为

此时消息的回执 (用于显示消息未读已读) 会产生大量写操作, 如果是旁路缓存的策略, **缓存将一直被删除失效**

可能会导致一些本来QPS比较低的业务, 由于大群的突发流量, 被打崩, 例如群里来了新成员是一个大V, 导致一群人去查看群成员列表, 然后去做加好友的操作

### 大群优化策略

大群共有M个人在线, 其中N个人在发消息(假设每人平均每秒发送X条消息), 那么就会存在每秒 $M\times N \times X$ 个消息收发, 引发消息风暴

优化方向:

把一部分消息($N \times X$)合并发送给用户, 采用推拉结合的策略

1. 服务端接收到这个群组发来的多个消息, 在一定时间内进行整理和排序后放入DB(for example)
2. 向需要接收消息的用户发送一个Pull请求
3. 用户向服务器发起请求, 拉取多个还未收到的消息, 降低了消息的传输

回执消息, 即报告此用户对此消息的已读状态, 在大群的场景下也涉及大量的写请求

- 回执消息的特性, 就是对一致性的要求不高, 可以异步进行
- 对聊天室进行热度的区分, 对于热度高的聊天室, 优先级降低, 服务降级, 例如从推模式转为拉模式

禁用心跳, 减少数据传输次数



对于大量信息, 用户一下子看不过来, 可能会向上拉, 于是可以采用预拉取:

- 客户端在拉取第一次信息包之后, 提前异步地去拉取几个下面的信息包
- 服务端也可以保留几个较旧的数据在缓存



## 是否保存数据

- 企业聊天
   - 应当保存较长的时间
- 用户的聊天信息
   - 考虑用户的隐私
   - 降低泄露的分享
   - 尽可能少地保存用户的聊天记录

可以把每条消息抽象成特征向量, 然后存储, 可用于后续的统计和分析, 而消除了个人的敏感信息(大概)





## API over WebSocket

已经建立了Websocket长连接了, 一般的请求能复用这个连接吗?

困难: WebSocket是有状态的, 不是请求响应模型的了, 因此此时需要自己维护整个请求响应模型, 要求请求和响应进行对应

**STOMP协议**，本质上是在WebSocket之上定义了一套文本协议，自然支持“请求-响应”模式



## 多端在线

对于路由的转发

- 自己发送的消息, 要同时推送到自己其他端所在的服务器上

对于顺序性的保证: 思路依旧是想办法从全局ID生成器里去正确地拿ID

- 应当看作不同的用户在发送消息, 那么用用户ID做区分的部分应当改成客户端的机器ID做区分了

- 那么即使在单点的聊天里, 也要看作群聊, 不同设备的同用户, 也要看作是另一个用户一样发送消息

## Pub/Sub路由

Pub/Sub是 Redis 的一种发布订阅模型, 基本的点对点消息模型

-   一个消费者可以订阅一个或多个channel(频道)
-   所有订阅者都能接收到订阅的channel的相关消息
-   天生就是阻塞式的

- 优点

   -   支持多生产, 多消费

- 缺点

   - 不支持数据持久化

     生产者不保存数据, 没有消费者消费消息, 消息将被丢失

   - 无法避免消息丢失

   - 消息堆积有上限, 超出时数据丢失

     消费者处理消息如果超时, 下一条消息纷至沓来, 超出上限就会丢失

对于Pub/Sub的消息丢失, 可以利用IM系统的Seq, 主动向生产者请求被丢失的数据

Redis的另外的实现有Stream, 但是Stream会存储消息(通过日志追加写), 偏重量, 效率偏低, 仅路由的话是不需要的



## 高可用

长连接存在的问题

长连接是跨越公网的

内网来说, 长连接断联的可能性低; 而经过NAT公网, 经过不同运营商的网络, 可能存在断联

### 心跳保活

- TCP的keepalive心跳机制
   - 默认两小时
   - 运营商会对长时间(5min)不收发消息的连接会进行强制断开
- 客户端来发送心跳
   - 服务端来发送心跳, 就要维护所有连接的定时任务, 几乎不可用
   - 服务端接收到心跳后, 对连接的计时器做一个重置
   - 如果服务端长时间收不到心跳, 就会把连接断开, 节省资源
- 控制心跳包的大小
   - 超时时间
   - 包是第几次重试
   - 用于网络状态的统计
- 权衡心跳的时间间隔
   - 心跳过长, 服务端感知断线客户端的效率越低, 资源利用率越低
   - 心跳过短, 造成**心跳潮汐**, 给网关造成流量压力
- 不同运营商的**NAT淘汰时间**不一样, 心跳应该比最短的那个时间小一些
   - 客户端采用固定心跳, 采用测算好了的值
   - 后台状态(服务之间的内网连接)采用自适应心跳包, 对NAT淘汰时间进行测算(二分法)
   - 引入随机值, 减少网路潮汐的可能性



### 断线重连

移动端在移动的时候可能切换网络, 导致**断线重连**

希望重连之后能够连接在同一个节点上, 因为节点上的有关数据不一定是第一时间进行删除的(如果第一时间删除对CPU的消耗较大). 希望能够复用之前连接的有关信息



如果移动速度较快, 可能导致**发生多次断线重连**

可以让客户端将几次快速的重连合并成最终一次, 控制客户端在一段时间内仅发起一次重连请求, 减轻服务端的压力

“节流”或“防抖”的思想

- 节流: 限制函数在一定时间间隔内执行一次，无论事件触发有多频繁。
- 防抖: 将多次执行合并为一次，只有在最后一次触发事件后，延迟一定时间没有新的触发，才执行操作。

或者**退化成短链接**, 去拉取信息而不是收发即时消息





**服务端宕机**导致大量客户端重新连接到另一台机器上(另一台机器要突然处理这么多重连, 也可能被压垮)

解决方案是引入所有IMServer节点的注册中心, 能及时发现有节点宕机并能够将重连的请求随机打散



### 消息风暴

websocket作为有状态的长连接, 如果服务上关联了大量连接, 随之而来的是大量对连接状态的管理

如果服务宕机, 导致大量连接状态的丢失, 那么重连之后需要**重新建立状态**, 造成性能的损耗

- 引入 State 服务, 专门用于存储连接的状态, 与websocket直接连接的服务不存储 State  信息
- State 可以做一定持久化处理, 比如使用**快照+Checkpoint**
- 但是恢复的时候, 对时效性很强的信息(比如计时器), 无法恢复完全



大量定时器占用大量内存资源, 并且定时任务的触发会使得整个系统**卡顿造成消息超时**

- 传统的定时任务采用二叉堆实现(小根堆, 获取最近的任务然后等待, 执行), 增删的复杂度是LogN

- 采用**时间轮**, 可以常量级别的插入和删除, 但定时精度有所丢失(业务不太需要太高精度)

   - 认为任务的延时分布是均匀的
   - 时间轮有固定个数的插槽, 每个插槽是链表
   - 链表上有多个节点存储任务和"**圈数**"

  插入

   1. 新增任务时执行一次 **运算** 找到插槽

      计算出槽位的偏移量
      $$
      任务延时跨越的槽位数量=\frac{任务的延迟时间}{每个槽位的时间跨度}
      $$
      计算出槽位
      $$
      槽位 = 任务延时跨越的槽位数量\mod 总槽位数
      $$

2. 记录圈数为
   $$
   圈数 = \frac{当前插槽+任务延时跨越的槽位数量}{总槽位数}
   $$

3. 将节点放入插槽

查找下一个任务

1. 指针往下一个插槽移动
2. 遍历插槽的链表, 执行此时插槽中所有圈数为0的任务
3. 指针每转过一圈，所有任务的圈数减1。

放入和删除都是O(1)的




# Timeline Feed

## Feed 概述

### 目标

用于推送

- 信息单元(商品/视频/图文)
- 广告



### 要求

- 高可用
- 极低延迟

### 要素

- 召回: 决定哪些Feed应该分发给哪些用户

- 排序: 区分展示信息的优先级. 排序规则, 商业性更高



### 分类

Timeline Feed:

- 通过用户与用户之间的关注关系来召回Feed

- 基于发布时间排序

Top k Feed:

- 依据某些召回策略来召回Feed

  召回策略

   - HNSW 向量检索

     构建向量点的图, 要求构成联通图, 边尽可能少, 相近的点之间能够直接向量

- 依据推荐模型排序

   - 对点击率的预估做排序
   - 机器学习的模型, 概率有多高, 广告转化率, 贝叶斯

- 复杂的推荐系统



## 设计目标

- 用户发布Feed
- 用户关注/取消关注其他用户
- 用户查看订阅频道, 看到关注的用户发布的Feed, 以按照时间排序
- 用户可以点击某个Feed, 进入到当前Feed的详情页面
- 用户可以进入某个用户的个人主页, 展示其曾经发布的Feed
- 用户的Feed有标题/封面/点赞/评论/流量数等信息
- 用户可以点赞评论转发一个Feed
- 用户可以看自己的收藏/点赞/浏览记录列表, 并~~设置是否公开权限~~
- ~~需要审核机制~~



## 指标

- 延迟
- 吞吐
- 可用性



## 发布Feed

### 推拉模式

推模式/写扩散: 创作者发布Feed后, 将Feed写入每一个关注者的inbox中(适合在小规模社交系统上进行, 或者是朋友圈这种无法形成大规模粉丝群体的场景)

拉模式: 创作者发布Feed后, 将Feed写入自己的outbox, 关注者想要相关信息即可从创作者的outbook中获取

推拉结合

- 创作者

   - 普通创作者: 粉丝量少

     策略: 直接采取推模式, 写给每一个用户的inbox

   - 大V: 粉丝量多

     策略: 对于核心粉丝, 推模式写inbox; 对于普通粉丝, 采取拉模式, 写自己的outbox, 让粉丝到自己的outbox去获取

- 关注者

   - 活跃粉丝: 对创作者的Feed请求频率高. 在粉丝数量中占比少.
   - 普通粉丝: 对创作者的Feed请求频率低, 偶尔请求. 在粉丝数量中占比多.
   - 读取时, 读取自己的关注列表, 获取信息(去inbox读还是去outbox读),  然后并行去请求信息, 最后合并成结果

**数据结构**: 采取以timestamp为score的跳表



识别大V

- 粉丝数量

- 舆情热度(某个人物在某个特定的热点事件中成为了热点人物)计算

- 机器学习预测

- 大 V 不能进行降级

  如果进行降级:

   - 需要对所有的粉丝的inbox进行检查
   - 将大V的outbox中的数据填入粉丝的inbox
   - 删除大V的outbox

- 大V可以分等级, 控制采取推模式的活跃用户占比多少

识别活跃用户

- 活跃程度不同, 收件箱的大小也不同
-





### 滚动查询

客户端采用滚动更新查看的样式

下一次请求带上上一次last_id, 下一次查询从last_id开始查询



### 下拉刷新

将last_id置为空, 表示是查询最新的Feed





### 冷热分离

- 如果用户持续获取Feed, 直到inbox用完, 此时退化成关注者的outbox拉取



### 预拉取



- 在下拉刷新到第9次(假设), 接近将本用户的收件箱拉完了, 服务端进行下10次的拉取
- 因为拉取到9次及以上, 概率很低, 因此触发预拉取的QPS也不高, 所以资源消耗的成本可以接受





### 问题

- 如何识别大V用户避免边界问题导致的性能抖动?

   - 使用分级, 阶段性地进行转变

- 大V发布Feed, 所有人去拉大V的最新Feed, 此时大V的发件箱所在的服务器宕机?

   - 建立高可用的集群
   - 读写分离/分片 Redis slot 插槽

- 用户关注的创作者过多, 导致收件箱列表过大, 造成大Key问题?

   - 收件箱只存储近期的Feed, 限制大小
   - 采用推拉结合, 不是常关注的大V, 采用拉模式
   - 采用预拉取的方式

- 对于推模式, 如果一个Feed被删除, 如何快速更新?

   - 由于升级成了推拉结合, 所以**扇出放大**的影响收到一定控制

   - 并发删除创作者的outbox的Feed和粉丝的inbox的Feed, 粉丝的部分可以采用合适的batch

   - 引入一个新的黑名单, 创作者删除时优先将Feed标记到黑名单

     在获取Feed时, 即使按照前面的逻辑获取到了Feed,  再次经过黑名单的检查后, 过滤掉黑名单Feed, 才能返还给客户端

     在inbox和outbox的Feed都确认删除后, 从黑名单中删除Feed

   - 或者惰性删除, 优先删除活跃用户的inbox, 对于非活跃用户, 降低删除优先级, 甚至放到在下一次登录/查看时检查inbox, 顺便进行删除

- 收件箱如果写入失败如何处理?

   - 重试

   - MQ 来提高吞吐量并解耦合

     (本来是写缓存的业务, 多一个MQ, 增加的网络IO是否会导致累赘呢? 如果还不存在需求就不要引入MQ吧)



RocksDB(ali), KV存储引擎.

- 依旧遵守Redis的协议, 对使用者透明
- 内部基于磁盘
- 性能的部分丢失
- 高效读写权衡, 使用磁盘, 大量分担内存压力, 极大降低硬件成本





# 聚合打包服务

## 详情页设计

### Feed 流的消费数据

粉丝收到热门 Feed, 大量点击阅读, **进入详情页**, 使得详情页QPS拉高, 多种下游服务(点赞数, 评论列表, 收藏数等)的整体QPS拉高

导致系统性崩溃, 服务不可用

同理还有导致评论数激增, 或产生大量关注行为

### 快速收回

Feed已经推送给大量用户, 但是这个Feed不愿意让用户看到(例如发出的Feed是假新闻), 需要快速收回

进行Delete操作太过耗时(比如有时由于多级缓存等机制, 无法保证快速删除全域的所有数据), 无法快速收回, 减少损失

方案是缓存一个能够快速写的黑名单, 即使拿到了Feed, 进入详情页时要检查这个黑名单, 发现Feed流在黑名单里, 则快速响应给用户404

## 聚合打包服务

详情页的工作, 需要聚合多个信息

- 创作者的用户信息(当前用户是否关注该创作者的关系信息)
- 点赞数/收藏数/分享数
- 观看量
- 评论数
- 评论列表

## 旁路缓存

### 缓存三剑客

#### 击穿

- 缓存不存在数据, 大量数据打入数据库
- 互斥锁(引分布式锁)
- Redission看门狗机制, 续时分布式锁
- value 一般是UUID表进程+Thread_ID 表进程
- value 采用 hash 用以实现可重入锁

#### 穿透

- 访问不存在的数据, 导致永远无法缓存, 请求永远打到数据库

- 发现这种请求, 就构造一个空数据到缓存, 下一次访问时访问这个空数据

- 布隆过滤器(insert on 数据库), 假阳性, 布隆过滤器说存在, 可能不存在; 说不存在, 一定不存在.

  升级: 布谷鸟过滤器(有利于减少Hash冲突)

#### 雪崩

- 宕机导致: 高可用集群
- 大量key同时过期导致: 过期时间引入随机值



### 一致性

- 为什么删除缓存而不是更新?: 更改成本高不稳定, 两个线程先后改缓存,  顺序一变结果不一样

- 为什么先写数据库再删缓存?: 如果先删缓存, 别的线程来请求数据库放到缓存, 缓存依旧旧数据

- 先写数据库再删缓存导致的数据不一致?: 时间较短可以接受

- 如何解决删除缓存的耦合问题?: 使用canal, 监听binlog, 如果发现写操作完成后发消息解耦

- 数据库主从导致缓存不一致的情况: 主库写成功, 删除缓存, 但从库没完成同步, 另一个线程自从库读取到旧数据, 加载旧数据到缓存

  解决方案:

   - 同步双删, 完成写操作后删除一次缓存, 一定时间后再去删除一次缓存, 保证缓存的旧数据不会存在太久(间隔时间难以确认)
   - 在主从之间引入全同步/半同步, 使用ACK来保证数据一致性(性能下降)

#### 缓存不一致场景

缓存不一致的场景1

1. 缓存中无数据, 数据库中有数据V1
2. 服务A从缓存中读数据发现无数据于是从数据库中读出V1
3. 服务B讲数据库中的数据改成V2, 删除一次缓存
4. 服务A将V1写入缓存
5. 此时, 缓存存储数据V1, 数据库存储数据VB

canal 来解耦后就没有这个问题了, 因为是统一用canal来写缓存而不是用来服务缓存

话说服务A要去读数据库的数据V1, 加了==分布式锁(之前用于解决击穿)==, 服务B就无法去修改数据库的数据了, 所以这种情况不应该存在....?

缓存不一致问题2:

1. 缓存中无数据, 数据库中有数据V1
2. 服务A从缓存中读不到数据, 从数据库中读取到数据V1
3. 服务B更改数据库的数据V1到V2, 删除缓存
4. 服务C从发现缓存中不存在数据, 去读数据库, 读到V2, 写入缓存
5. 服务A将V1写入缓存
6. 此时缓存的数据是V1, 数据库的数据是V2, 产生不一致的问题

引入==分布式锁(之前用于解决击穿)== 或者 canal 之后, 这种缓存不一致的情况也不存在

#### 租约

Facebook 的解决方案: **引入租约**

租约解决缓存不一致问题1:

1. 缓存为空, 数据库中有数据V1
2. 服务A发现缓存为空, 就在缓存中记录一个token1, 自己保存这个token1, 然后去数据库读V1
3. 服务B修改数据库的数据V1到V2, 然后删除缓存(同时删除token)
4. 服务A去写缓存, 发现缓存中的token1不存在, 更换一个token, token2写入缓存, 自己存这个token2, 然后去数据库读V2
5. 服务A去写缓存, 发现缓存token2和自己的token2一致, 于是写缓存V2, 删除缓存中token2的记录

租约解决缓存不一致问题2:

1. 缓存为空, 数据库中有数据V1
2. 服务A发现缓存为空, 就在缓存中记录一个token1, 自己保存这个token1, 然后去数据库读V1
3. 服务B更改数据库的数据V1到V2, 删除缓存
4. 服务C发现缓存为空, 就在缓存中记录一个token2, 自己保存这个token2, 然后去数据库读V2
5. 服务C去写缓存, 发现缓存token2和自己的token2一致, 于是写缓存V2, 删除缓存中token2的记录
6. 服务A写缓存, 发现缓存中没有token了, 于是知道缓存中的数据被更新到了更新的版本, 于是从缓存中取得V2

租约解决击穿:

1. 缓存为空, 数据库中有数据V1
2. 服务A发现缓存为空, 就在缓存中记录一个token1, 自己保存这个token1, 然后去数据库读V1
3. 服务B发现缓存为空, 发现记录了token1, 于是自己等待一会儿, 再去访问, 循环数次
4. 服务C同服务B
5. 服务A发现自己携带的token1和缓存中的token1一致, 成功将V1加载到缓存
6. 服务B和C发现缓存中没有token了, 且已经加载了数据, 就读取缓存

租约相对于分布式锁, 不会阻塞等待(分布式锁阻塞等待是使用了Redis的PubSub机制, 至少Redission是这么实现的), 但是会不断轮询重试.

当然分布式锁也可以设计成轮询重试的模式, 实现起来也比租约简单一些

### Redis 高可用

- 主从(哨兵集群)
- 分片
- 持久化



#### 持久化

RDB 快照

- 恢复快, 间隔长, 两次间隔之间的数据会丢失, 比较消耗CPU
- Fork页表时是阻塞的
- COW, Copy on Write. Fork页表是阻塞的



AOF 日志

- 指令完成后执行, 底层是fsync, 配置有always(阻塞下一次), everysec (丢失1s, 不阻塞), no



AOF文件重写机制

- 基于当前的Redis已有数据生成指令
- COW, Copy on Write. Fork页表是阻塞的

- AOF重写 对于 COW 时期的数据不一致如何处理?: 使用AOF缓冲区和AOF重写缓冲区, COW期间发生的变化会增量式地写入AOF重写后的文件



混合持久化

- 异步采用RDB
- 两次RDB之间(包括进行RDB时)采用AOF
- RDB完了之后追加AOF日志



AOF重写机制有 **AOF缓冲区** 和 **AOF重写缓冲区** 两个缓冲区, 为什么这么设计, 目的呢?

- AOF缓冲区+旧文件形成一个完整备份
- AOF重写缓冲区和重写的新文件形成一个完整备份
- 假设AOF重写失败, 依旧保证数据完整



Redis 7.0 之后删除AOF重写缓冲区, 两次重写之间和重写时的指令写入 `.incr.aof` 文件

1. 已经有文件 `1.base` 和 `1.incr.aof` 持久化了数据
2. 开启 `bgrewriteaof`
3. 创建 `2.incr.aof`, 停止写 `1.incr.aof`
4. 进行fork
5. 子进程异步将内存中的数据rewrite到 `2.base`
6. 主进程其他命令进入, 写入 `2.incr.aof`
7. 子进程完成 `2.base`, 删除 `1.incr.aof` 和 `1.base`
8. 主进程继续写入`2.incr.aof`

如果AOF重写失败, `1.base`, `1.incr.aof`, `2.incr.aof`将一起作为持久化的记录



#### 大Key问题

- 不仅是内存损耗, 还有输出缓冲区buffer(不受淘汰机制控制) 占满(阻塞网络, 阻塞主线程)
- 大key(Hash或集合这种)拆分, Lua进行聚合. (集群场景下, 希望落在同一个分片, 花括号)
- 分片



#### Hot Key 问题

引入**多级缓存**

- JVM 的 caffine
- Nginx缓存静态资源
- 前端缓存

**双TTL**, 用于提高可用性

- 一个小缓存内的数据采用一个短TTL, 例如10s, 保证数据较强一致性
- 一个大缓存内的数据采用长TTL, 例如24h.
- 小缓存的数据过期后, 进入大缓存

1. 当上游服务A发现小缓存内无数据时, 访问下游服务B获取数据
2. RPC的过程中失败了
3. 服务A去获取大缓存中的较旧数据, 依旧能组建数据



场景: 服务A的业务是设置ItemA, 有自己的缓存RedisA, 服务X的业务是聚合ItemA和ItemB. 服务X有自己的缓存RedisB. 为了提高效率, 减少RPC带来的性能损耗, 服务x将聚合的结果存入了RedisX缓存. 那么问题是, 一个修改ItemA的请求打入服务A, 且不经过服务X, 导致服务X对ItemA的变化不可见. 这怎么办?

难点: 前提是RedisA和RedisX的设置和部署只对对应服务的开发者才了解, 其他开发者, 是不了解的, 相当于一个黑盒. 服务A对ItemA进行修改后如果要去修改RedisX的数据, 需要知道服务X的一系列部署细节, 明显是耦合了, 非常不合适.

解决方案: 使用消息队列, 订阅这个消息, 只要这个项目在项目初期就做好规划和约定, 在合适的地方监听消息队列的消息, 以及时保证缓存的数据一致性. 使用消息队列能够高度解耦, 而不需要知道其他业务的细节

## 客户端预加载

对于用户停留在某个Feed流的时候, 客户端异步请求下两次(或多次)Feed流, 并保存在客户端内存 提高用户的体验

坏处: 造成了无效的请求

## 无效下游请求

假设详情页服务异步地去请求多个服务, 某个关键服务快速失败, 此时认为整个请求失败.

但是RPC对其他服务的请求还是会继续执行下去, 其他服务还有更多子服务, **导致RPC资源的浪费**



**解决方法**是希望发现请求失败的时候快速通知其他请求, 统统尽快结束, 避免对资源的占用

如何找到需要打断的目标? 使用链路追踪的技术

如何打断正在进行的服务?

假设服务A同时调用服务B和服务C, 服务A在阻塞等待两个服务的结果, 服务B和服务C在执行中

如果服务B和服务C接受到了打断通知, 正确打断之后, 是否需要通知服务A此服务被打断? 不需要, 这个时候说不定服务A已经被打断了

那么, 可以异步地打断服务ABC, 然后不需要服务ABC进行返回.



## 频繁写优化

比如点赞数和观看量, 就是会被频繁写的场景, 但其结构又很简单, 只是一个计数

写操作吞吐优先: 要能处理大量的写请求

读操作延迟优先: 不需要读到最新的数据

方案:

- 抽象出一个CounterServer, 用于这种业务简单, 但是要求吞吐极大的场景
- **变更合并策略**: 引入MQ削峰聚合, 比如100次观看数加一的请求, 合并成1次加100的请求



# Elasticsearch 复杂查询

需求分析:

需要对标题进行分词查询, 进行复杂查询, 排序的结果是需要综合考虑多方面的

- 关键词的匹配度
- 作品的热门程度, 以点击量为例
- 作品发布的时间
- 用户喜好的向量

可能出现的问题:

- 数据同步
   - 简单使用双写来实现: 代码侵入, 效率低
   - 异步双写, 引入消息队列: 解耦
   - 引入Canal 最终同步
- 搭建集群, 一主多从:
   - 脑裂: 选举票超过半数, 裂成两半, 节点多的哪个集群产生下一个主

Elasticsearch 的集群和分片

就是一台机器上存储多个分片, 分片存在冗余

Elasticsearch A 存在分片 1 2

Elasticsearch B 存在分片 2 3

Elasticsearch C 存在分片 1 3

那么任意机器宕机, 都保证能够复原所有分片的数据, 保证至少有一个分片的数据还在



# 总结

## 分级存储架构

- Nginx(CDN静态资源存储)
- JVM(Caffine)
- 双TTL缓存
   - Redis(小缓存)
   - Redis(大缓存, 甚至RocksDB)
- Elasticsearch(复杂查询)
- NoSQL(快速存储, 注重读写, 容许弱一致性,  最终一致)
- 关系型数据库(最终落库, 不在意效率, 允许较大事务, 在意一致性)

